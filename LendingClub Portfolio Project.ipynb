{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Goal\n",
    "\n",
    "Given historical data on loans given out with information on whether or not the borrower defaulted (charge-off), build a model that can predict whether or not a borrower will pay back their loan. Then, in the future when the company gets a new potential customer, it can assess whether or not that customer is likely to pay back the loan.\n",
    "\n",
    "The \"loan_status\" column contains the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the LendingClub data info\n",
    "It's not very clear what some of the column names mean, so create a function that returns a longer description of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_info = pd.read_csv('Data/lending_club_info.csv', index_col='LoanStatNew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the description of a feature\n",
    "\n",
    "def get_feature_info(feature_name):\n",
    "    print(data_info.loc[feature_name]['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\n"
     ]
    }
   ],
   "source": [
    "get_feature_info('revol_util')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the actual LendingClub data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/lending_club_loan_two.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>...</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>initial_list_status</th>\n",
       "      <th>application_type</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>11.44</td>\n",
       "      <td>329.48</td>\n",
       "      <td>B</td>\n",
       "      <td>B4</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>117000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36369.0</td>\n",
       "      <td>41.8</td>\n",
       "      <td>25.0</td>\n",
       "      <td>w</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0174 Michelle Gateway\\nMendozaberg, OK 22690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>11.99</td>\n",
       "      <td>265.68</td>\n",
       "      <td>B</td>\n",
       "      <td>B5</td>\n",
       "      <td>Credit analyst</td>\n",
       "      <td>4 years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20131.0</td>\n",
       "      <td>53.3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>f</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1076 Carney Fort Apt. 347\\nLoganmouth, SD 05113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15600.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>10.49</td>\n",
       "      <td>506.97</td>\n",
       "      <td>B</td>\n",
       "      <td>B3</td>\n",
       "      <td>Statistician</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>RENT</td>\n",
       "      <td>43057.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11987.0</td>\n",
       "      <td>92.2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>f</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87025 Mark Dale Apt. 269\\nNew Sabrina, WV 05113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7200.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>6.49</td>\n",
       "      <td>220.65</td>\n",
       "      <td>A</td>\n",
       "      <td>A2</td>\n",
       "      <td>Client Advocate</td>\n",
       "      <td>6 years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5472.0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>f</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>823 Reid Ford\\nDelacruzside, MA 00813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24375.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>17.27</td>\n",
       "      <td>609.33</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>Destiny Management Inc.</td>\n",
       "      <td>9 years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24584.0</td>\n",
       "      <td>69.8</td>\n",
       "      <td>43.0</td>\n",
       "      <td>f</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>679 Luna Roads\\nGreggshire, VA 11650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt        term  int_rate  installment grade sub_grade  \\\n",
       "0    10000.0   36 months     11.44       329.48     B        B4   \n",
       "1     8000.0   36 months     11.99       265.68     B        B5   \n",
       "2    15600.0   36 months     10.49       506.97     B        B3   \n",
       "3     7200.0   36 months      6.49       220.65     A        A2   \n",
       "4    24375.0   60 months     17.27       609.33     C        C5   \n",
       "\n",
       "                 emp_title emp_length home_ownership  annual_inc  ...  \\\n",
       "0                Marketing  10+ years           RENT    117000.0  ...   \n",
       "1          Credit analyst     4 years       MORTGAGE     65000.0  ...   \n",
       "2             Statistician   < 1 year           RENT     43057.0  ...   \n",
       "3          Client Advocate    6 years           RENT     54000.0  ...   \n",
       "4  Destiny Management Inc.    9 years       MORTGAGE     55000.0  ...   \n",
       "\n",
       "  open_acc pub_rec revol_bal revol_util total_acc  initial_list_status  \\\n",
       "0     16.0     0.0   36369.0       41.8      25.0                    w   \n",
       "1     17.0     0.0   20131.0       53.3      27.0                    f   \n",
       "2     13.0     0.0   11987.0       92.2      26.0                    f   \n",
       "3      6.0     0.0    5472.0       21.5      13.0                    f   \n",
       "4     13.0     0.0   24584.0       69.8      43.0                    f   \n",
       "\n",
       "  application_type  mort_acc  pub_rec_bankruptcies  \\\n",
       "0       INDIVIDUAL       0.0                   0.0   \n",
       "1       INDIVIDUAL       3.0                   0.0   \n",
       "2       INDIVIDUAL       0.0                   0.0   \n",
       "3       INDIVIDUAL       0.0                   0.0   \n",
       "4       INDIVIDUAL       1.0                   0.0   \n",
       "\n",
       "                                           address  \n",
       "0     0174 Michelle Gateway\\nMendozaberg, OK 22690  \n",
       "1  1076 Carney Fort Apt. 347\\nLoganmouth, SD 05113  \n",
       "2  87025 Mark Dale Apt. 269\\nNew Sabrina, WV 05113  \n",
       "3            823 Reid Ford\\nDelacruzside, MA 00813  \n",
       "4             679 Luna Roads\\nGreggshire, VA 11650  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *The target variable is currently represented as 'Fully Paid' or 'Charge Off'.  Change these target labels to binary ints.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fully Paid     318357\n",
       "Charged Off     77673\n",
       "Name: loan_status, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 means the individual paid the loan; 1 means the individual defaulted\n",
    "df['loan_status'] = df['loan_status'].apply(lambda status: 0 if status == 'Fully Paid' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['loan_status'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, deal with missing data.\n",
    "I will investigate the features that contain null values to determine if they should be dropped, filled, or used to create other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_amnt                   0\n",
       "term                        0\n",
       "int_rate                    0\n",
       "installment                 0\n",
       "grade                       0\n",
       "sub_grade                   0\n",
       "emp_title               22927\n",
       "emp_length              18301\n",
       "home_ownership              0\n",
       "annual_inc                  0\n",
       "verification_status         0\n",
       "issue_d                     0\n",
       "loan_status                 0\n",
       "purpose                     0\n",
       "title                    1755\n",
       "dti                         0\n",
       "earliest_cr_line            0\n",
       "open_acc                    0\n",
       "pub_rec                     0\n",
       "revol_bal                   0\n",
       "revol_util                276\n",
       "total_acc                   0\n",
       "initial_list_status         0\n",
       "application_type            0\n",
       "mort_acc                37795\n",
       "pub_rec_bankruptcies      535\n",
       "address                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show which features contain null values, and if they do, how many\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The job title supplied by the Borrower when applying for the loan.*\n",
      "\n",
      "\n",
      "Teacher                             4389\n",
      "Manager                             4250\n",
      "Registered Nurse                    1856\n",
      "RN                                  1846\n",
      "Supervisor                          1830\n",
      "                                    ... \n",
      "Aston Hotels & Resorts                 1\n",
      "William Morris                         1\n",
      "Zeno Office Solutions                  1\n",
      "court bailiff                          1\n",
      "Senior Claims Benefit Specialist       1\n",
      "Name: emp_title, Length: 173105, dtype: int64\n",
      "\n",
      "\n",
      "# of null entries:  22927\n"
     ]
    }
   ],
   "source": [
    "# Investigate 'employment title'\n",
    "get_feature_info('emp_title')\n",
    "print('\\n')\n",
    "print(df['emp_title'].value_counts())\n",
    "print('\\n')\n",
    "print('# of null entries: ', df['emp_title'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because there are so many null entries and so many unique job titles, I will create a binary variable called \"emp_title_known\" that will indicate whether or not the applicant's job title is known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 for job title known; 0 for unknown\n",
    "df['emp_title_known'] = df['emp_title'].apply(lambda title: 1 if type(title) == str else 0)\n",
    "\n",
    "# Drop the old column\n",
    "df.drop('emp_title', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>...</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>initial_list_status</th>\n",
       "      <th>application_type</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>address</th>\n",
       "      <th>emp_title_known</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>11.44</td>\n",
       "      <td>329.48</td>\n",
       "      <td>B</td>\n",
       "      <td>B4</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>117000.0</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36369.0</td>\n",
       "      <td>41.8</td>\n",
       "      <td>25.0</td>\n",
       "      <td>w</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0174 Michelle Gateway\\nMendozaberg, OK 22690</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt        term  int_rate  installment grade sub_grade emp_length  \\\n",
       "0    10000.0   36 months     11.44       329.48     B        B4  10+ years   \n",
       "\n",
       "  home_ownership  annual_inc verification_status  ... pub_rec  revol_bal  \\\n",
       "0           RENT    117000.0        Not Verified  ...     0.0    36369.0   \n",
       "\n",
       "  revol_util total_acc  initial_list_status application_type  mort_acc  \\\n",
       "0       41.8      25.0                    w       INDIVIDUAL       0.0   \n",
       "\n",
       "   pub_rec_bankruptcies                                       address  \\\n",
       "0                   0.0  0174 Michelle Gateway\\nMendozaberg, OK 22690   \n",
       "\n",
       "   emp_title_known  \n",
       "0                1  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10+ years    126041\n",
      "2 years       35827\n",
      "< 1 year      31725\n",
      "3 years       31665\n",
      "5 years       26495\n",
      "1 year        25882\n",
      "4 years       23952\n",
      "6 years       20841\n",
      "7 years       20819\n",
      "8 years       19168\n",
      "9 years       15314\n",
      "Name: emp_length, dtype: int64\n",
      "\n",
      "\n",
      "18301\n"
     ]
    }
   ],
   "source": [
    "# Investigate \"emp_length\"\n",
    "print(df['emp_length'].value_counts())\n",
    "print('\\n')\n",
    "print(df['emp_length'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are only 11 unique categories for the \"emp_length\" variable, so it's feasible to create dummy variables out of them. However, first investigate the ratio of default to fully paid grouped by employment length to determine if there is sufficient variance in this feature to keep it around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_co = df[df['loan_status'] == 0].groupby('emp_length').count()['loan_status']\n",
    "emp_fp = df[df['loan_status'] == 1].groupby('emp_length').count()['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_len = emp_co / emp_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='emp_length'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp_len.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There's not much of a difference between the default-to-paid ratios across different employment length groups. Thus, I will drop the 'emp_length' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('emp_length', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_amnt                   0\n",
       "term                        0\n",
       "int_rate                    0\n",
       "installment                 0\n",
       "grade                       0\n",
       "sub_grade                   0\n",
       "home_ownership              0\n",
       "annual_inc                  0\n",
       "verification_status         0\n",
       "issue_d                     0\n",
       "loan_status                 0\n",
       "purpose                     0\n",
       "title                    1755\n",
       "dti                         0\n",
       "earliest_cr_line            0\n",
       "open_acc                    0\n",
       "pub_rec                     0\n",
       "revol_bal                   0\n",
       "revol_util                276\n",
       "total_acc                   0\n",
       "initial_list_status         0\n",
       "application_type            0\n",
       "mort_acc                37795\n",
       "pub_rec_bankruptcies      535\n",
       "address                     0\n",
       "emp_title_known             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loan title provided by the borrower\n"
     ]
    }
   ],
   "source": [
    "# Investigate \"title\"\n",
    "get_feature_info('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A category provided by the borrower for the loan request. \n"
     ]
    }
   ],
   "source": [
    "# The description above sounds similar to the purpose of a loan.\n",
    "get_feature_info('purpose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Debt consolidation               152472\n",
       "Credit card refinancing           51487\n",
       "Home improvement                  15264\n",
       "Other                             12930\n",
       "Debt Consolidation                11608\n",
       "                                  ...  \n",
       "To get rid of those things            1\n",
       "debt consolidation #1                 1\n",
       "credit cards/home improvement         1\n",
       "Kangen water appliance                1\n",
       "Connor improvements                   1\n",
       "Name: title, Length: 48817, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "debt_consolidation    234507\n",
       "credit_card            83019\n",
       "home_improvement       24030\n",
       "other                  21185\n",
       "major_purchase          8790\n",
       "small_business          5701\n",
       "car                     4697\n",
       "medical                 4196\n",
       "moving                  2854\n",
       "vacation                2452\n",
       "house                   2201\n",
       "wedding                 1812\n",
       "renewable_energy         329\n",
       "educational              257\n",
       "Name: purpose, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['purpose'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"title\" turns out to be a repeat of \"purpose\" but with null values. Drop the 'title' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('title', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_amnt                   0\n",
       "term                        0\n",
       "int_rate                    0\n",
       "installment                 0\n",
       "grade                       0\n",
       "sub_grade                   0\n",
       "home_ownership              0\n",
       "annual_inc                  0\n",
       "verification_status         0\n",
       "issue_d                     0\n",
       "loan_status                 0\n",
       "purpose                     0\n",
       "dti                         0\n",
       "earliest_cr_line            0\n",
       "open_acc                    0\n",
       "pub_rec                     0\n",
       "revol_bal                   0\n",
       "revol_util                276\n",
       "total_acc                   0\n",
       "initial_list_status         0\n",
       "application_type            0\n",
       "mort_acc                37795\n",
       "pub_rec_bankruptcies      535\n",
       "address                     0\n",
       "emp_title_known             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### revol_bal and pub_rec_bankruptcies have very few null values. See what fraction of total values are null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006969169002348307"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate \"revolv_util\"\n",
    "df.isnull().sum()['revol_util'] / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0013509077595131682"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate \"pub_rec_bankruptcies\"\n",
    "df.isnull().sum()['pub_rec_bankruptcies'] / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because revolv_util and pub_rec_bankruptcies both have such a small percentages of instances that are null, I'll just drop those null instances. This will remove a small number of rows from my dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['revol_util', 'pub_rec_bankruptcies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_amnt                   0\n",
       "term                        0\n",
       "int_rate                    0\n",
       "installment                 0\n",
       "grade                       0\n",
       "sub_grade                   0\n",
       "home_ownership              0\n",
       "annual_inc                  0\n",
       "verification_status         0\n",
       "issue_d                     0\n",
       "loan_status                 0\n",
       "purpose                     0\n",
       "dti                         0\n",
       "earliest_cr_line            0\n",
       "open_acc                    0\n",
       "pub_rec                     0\n",
       "revol_bal                   0\n",
       "revol_util                  0\n",
       "total_acc                   0\n",
       "initial_list_status         0\n",
       "application_type            0\n",
       "mort_acc                37205\n",
       "pub_rec_bankruptcies        0\n",
       "address                     0\n",
       "emp_title_known             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mortgage accounts.\n"
     ]
    }
   ],
   "source": [
    "# Investigate \"mort_acc\"\n",
    "get_feature_info('mort_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0     139708\n",
       "1.0      60384\n",
       "2.0      49907\n",
       "3.0      38022\n",
       "4.0      27865\n",
       "5.0      18182\n",
       "6.0      11059\n",
       "7.0       6050\n",
       "8.0       3120\n",
       "9.0       1653\n",
       "10.0       863\n",
       "11.0       479\n",
       "12.0       264\n",
       "13.0       146\n",
       "14.0       107\n",
       "15.0        61\n",
       "16.0        37\n",
       "17.0        22\n",
       "18.0        18\n",
       "19.0        15\n",
       "20.0        13\n",
       "24.0        10\n",
       "22.0         7\n",
       "21.0         4\n",
       "25.0         4\n",
       "27.0         3\n",
       "23.0         2\n",
       "32.0         2\n",
       "26.0         2\n",
       "31.0         2\n",
       "30.0         1\n",
       "28.0         1\n",
       "34.0         1\n",
       "Name: mort_acc, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['mort_acc'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is a lot of missing data and the range of this feature's values is vast, so simply filling the missing values with the average value of the feature probably won't work. One thing I could do is create a linear regression that uses the other numerical features to predict the missing mort_acc values. A simpler solution would be to find the feature that mort_acc is most highly correlated with and just use that feature to impute the missing mort_acc values. I will take the latter approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mort_acc                1.000000\n",
       "total_acc               0.381205\n",
       "annual_inc              0.236277\n",
       "loan_amnt               0.222381\n",
       "revol_bal               0.195063\n",
       "installment             0.193752\n",
       "open_acc                0.109440\n",
       "pub_rec_bankruptcies    0.027273\n",
       "pub_rec                 0.011576\n",
       "revol_util              0.007514\n",
       "emp_title_known        -0.021722\n",
       "dti                    -0.025401\n",
       "loan_status            -0.073048\n",
       "int_rate               -0.082656\n",
       "Name: mort_acc, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()['mort_acc'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'total_acc' is the feature most correlated with mort_acc. Therefore, I will group the dataframe by total_acc and for each of these total_acc groups, calculate the mean mort_acc. Then I can use this data as a lookup table to impute missing mort_acc values. More concretely, to impute a missing mort_acc value, I will simply take that instance's total_acc value and look up the associated average mort_acc value in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of credit lines currently in the borrower's credit file\n"
     ]
    }
   ],
   "source": [
    "get_feature_info('total_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.0     14257\n",
       "22.0     14235\n",
       "20.0     14193\n",
       "23.0     13899\n",
       "24.0     13855\n",
       "         ...  \n",
       "100.0        1\n",
       "115.0        1\n",
       "118.0        1\n",
       "108.0        1\n",
       "104.0        1\n",
       "Name: total_acc, Length: 118, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['total_acc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mort_acc_impute_table = df.groupby('total_acc').mean()['mort_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be applied to the mort_acc column.\n",
    "def fill_missing_mort_acc(total_acc, mort_acc):\n",
    "    if np.isnan(mort_acc):\n",
    "        return mort_acc_impute_table[total_acc]\n",
    "    else:\n",
    "        return mort_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8991868569532029"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mort_acc_impute_table[df.loc[0]['total_acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the function to the \"mort_acc\" column\n",
    "df['mort_acc'] = df.apply(lambda x: fill_missing_mort_acc(x['total_acc'], x['mort_acc']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_amnt               0\n",
       "term                    0\n",
       "int_rate                0\n",
       "installment             0\n",
       "grade                   0\n",
       "sub_grade               0\n",
       "home_ownership          0\n",
       "annual_inc              0\n",
       "verification_status     0\n",
       "issue_d                 0\n",
       "loan_status             0\n",
       "purpose                 0\n",
       "dti                     0\n",
       "earliest_cr_line        0\n",
       "open_acc                0\n",
       "pub_rec                 0\n",
       "revol_bal               0\n",
       "revol_util              0\n",
       "total_acc               0\n",
       "initial_list_status     0\n",
       "application_type        0\n",
       "mort_acc                0\n",
       "pub_rec_bankruptcies    0\n",
       "address                 0\n",
       "emp_title_known         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data now contains only non-null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My next task will be to convert numerical columns in non-numerical form into purely numerical form. I will also numerically encode categorical columns using dummy variables or an ordinal encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['term', 'grade', 'sub_grade', 'home_ownership', 'verification_status',\n",
       "       'issue_d', 'purpose', 'earliest_cr_line', 'initial_list_status',\n",
       "       'application_type', 'address'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the features that are non-numerical\n",
    "df.select_dtypes(['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 36 months    301247\n",
       " 60 months     93972\n",
       "Name: term, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investiage \"term\"\n",
    "df['term'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Although \"term\" is a categorical variable, it can be represented as a number and has a natural ordinal encoding. So I will simply change the strings to ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change these strings into ints\n",
    "df['term'] = df['term'].apply(lambda term: int(term.strip()[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36    301247\n",
       "60     93972\n",
       "Name: term, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['term'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B    115812\n",
       "C    105775\n",
       "A     64056\n",
       "D     63364\n",
       "E     31427\n",
       "F     11740\n",
       "G      3045\n",
       "Name: grade, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate \"grade\"\n",
    "df['grade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B3    26611\n",
       "B4    25558\n",
       "C1    23609\n",
       "C2    22541\n",
       "B2    22457\n",
       "B5    22046\n",
       "C3    21178\n",
       "C4    20232\n",
       "B1    19140\n",
       "A5    18500\n",
       "C5    18215\n",
       "D1    15947\n",
       "A4    15763\n",
       "D2    13916\n",
       "D3    12196\n",
       "D4    11625\n",
       "A3    10537\n",
       "A1     9717\n",
       "D5     9680\n",
       "A2     9539\n",
       "E1     7906\n",
       "E2     7410\n",
       "E3     6196\n",
       "E4     5354\n",
       "E5     4561\n",
       "F1     3530\n",
       "F2     2756\n",
       "F3     2277\n",
       "F4     1782\n",
       "F5     1395\n",
       "G1     1057\n",
       "G2      752\n",
       "G3      552\n",
       "G4      371\n",
       "G5      313\n",
       "Name: sub_grade, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate \"sub_grade\" variable\n",
    "df['sub_grade'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because the grade information is fully reprsented in the subgrade data, the grade feature is redundant. Thus, remove the grade column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop('grade', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Create dummy variables for the sub_grade column*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_grade_dummies = pd.get_dummies(df['sub_grade'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate these dummy variables to df and drop the old column\n",
    "df = pd.concat([df, sub_grade_dummies], axis=1)\n",
    "df.drop('sub_grade', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['loan_amnt', 'term', 'int_rate', 'installment', 'home_ownership',\n",
       "       'annual_inc', 'verification_status', 'issue_d', 'loan_status',\n",
       "       'purpose', 'dti', 'earliest_cr_line', 'open_acc', 'pub_rec',\n",
       "       'revol_bal', 'revol_util', 'total_acc', 'initial_list_status',\n",
       "       'application_type', 'mort_acc', 'pub_rec_bankruptcies', 'address',\n",
       "       'emp_title_known', 'A2', 'A3', 'A4', 'A5', 'B1', 'B2', 'B3', 'B4', 'B5',\n",
       "       'C1', 'C2', 'C3', 'C4', 'C5', 'D1', 'D2', 'D3', 'D4', 'D5', 'E1', 'E2',\n",
       "       'E3', 'E4', 'E5', 'F1', 'F2', 'F3', 'F4', 'F5', 'G1', 'G2', 'G3', 'G4',\n",
       "       'G5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns # Check the new column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MORTGAGE    198022\n",
       "RENT        159395\n",
       "OWN          37660\n",
       "OTHER          110\n",
       "NONE            29\n",
       "ANY              3\n",
       "Name: home_ownership, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate \"home_ownership\"\n",
    "df['home_ownership'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are few enough unique categories to create dummy variables. First, group 'NONE' and 'ANY' into 'OTHER', then create dummy variables for this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['home_ownership'] = df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MORTGAGE    198022\n",
       "RENT        159395\n",
       "OWN          37660\n",
       "OTHER          142\n",
       "Name: home_ownership, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['home_ownership'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_ownership_dummies = pd.get_dummies(df['home_ownership'], drop_first=True) # create dummies\n",
    "df = pd.concat([df, home_ownership_dummies], axis=1) # add dummies to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('home_ownership', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indicates if income was verified by LC, not verified, or if the income source was verified\n",
      "\n",
      "\n",
      "Verified           139451\n",
      "Source Verified    131301\n",
      "Not Verified       124467\n",
      "Name: verification_status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Investigate \"verification_status\"\n",
    "get_feature_info('verification_status')\n",
    "print('\\n')\n",
    "print(df['verification_status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are few enough unique categories, so create dummy variables\n",
    "verification_dummies = pd.get_dummies(df['verification_status'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('verification_status', axis=1, inplace=True)\n",
    "df = pd.concat([df, verification_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The month which the loan was funded\n"
     ]
    }
   ],
   "source": [
    "# Investigate \"issue_d\"\n",
    "get_feature_info('issue_d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You should definitely get rid of this feature because it indicates whether or not the loan was funded which we don't want to know when training the model. This is a form of data leakage, so remove this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('issue_d', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['purpose', 'earliest_cr_line', 'initial_list_status',\n",
       "       'application_type', 'address'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check which non-numerical variables remain\n",
    "df.select_dtypes(['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debt_consolidation    234169\n",
      "credit_card            82923\n",
      "home_improvement       23961\n",
      "other                  21059\n",
      "major_purchase          8756\n",
      "small_business          5656\n",
      "car                     4670\n",
      "medical                 4175\n",
      "moving                  2842\n",
      "vacation                2442\n",
      "house                   2197\n",
      "wedding                 1794\n",
      "renewable_energy         329\n",
      "educational              246\n",
      "Name: purpose, dtype: int64\n",
      "\n",
      "\n",
      "14  unique categories\n"
     ]
    }
   ],
   "source": [
    "# Investigate \"purpose\"\n",
    "print(df['purpose'].value_counts())\n",
    "print('\\n')\n",
    "print(df['purpose'].nunique(), ' unique categories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are few enough categories, so create dummy variables\n",
    "purpose_dummies = pd.get_dummies(df['purpose'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('purpose', axis=1, inplace=True)\n",
    "df = pd.concat([df, purpose_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['loan_amnt', 'term', 'int_rate', 'installment', 'annual_inc',\n",
       "       'loan_status', 'dti', 'earliest_cr_line', 'open_acc', 'pub_rec',\n",
       "       'revol_bal', 'revol_util', 'total_acc', 'initial_list_status',\n",
       "       'application_type', 'mort_acc', 'pub_rec_bankruptcies', 'address',\n",
       "       'emp_title_known', 'A2', 'A3', 'A4', 'A5', 'B1', 'B2', 'B3', 'B4', 'B5',\n",
       "       'C1', 'C2', 'C3', 'C4', 'C5', 'D1', 'D2', 'D3', 'D4', 'D5', 'E1', 'E2',\n",
       "       'E3', 'E4', 'E5', 'F1', 'F2', 'F3', 'F4', 'F5', 'G1', 'G2', 'G3', 'G4',\n",
       "       'G5', 'OTHER', 'OWN', 'RENT', 'Source Verified', 'Verified',\n",
       "       'credit_card', 'debt_consolidation', 'educational', 'home_improvement',\n",
       "       'house', 'major_purchase', 'medical', 'moving', 'other',\n",
       "       'renewable_energy', 'small_business', 'vacation', 'wedding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The month the borrower's earliest reported credit line was opened\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Oct-2000    3013\n",
       "Aug-2000    2930\n",
       "Oct-2001    2890\n",
       "Aug-2001    2879\n",
       "Nov-2000    2729\n",
       "            ... \n",
       "Jul-1958       1\n",
       "Jun-1955       1\n",
       "May-1962       1\n",
       "Jul-1951       1\n",
       "Feb-1961       1\n",
       "Name: earliest_cr_line, Length: 684, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate \"earliest_cr_line\"\n",
    "get_feature_info('earliest_cr_line')\n",
    "print('\\n')\n",
    "df['earliest_cr_line'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are too many unique Month-Year combinations. However, I can extract the year from these date strings and store them as ints so they can serve as ordinal categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1990"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(df['earliest_cr_line'][0].split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['earliest_cr_line'] = df['earliest_cr_line'].apply(lambda date: int(date.split('-')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['initial_list_status', 'application_type', 'address'], dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check which non-numerical variables remain\n",
    "df.select_dtypes(['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial listing status of the loan. Possible values are â€“ W, F\n"
     ]
    }
   ],
   "source": [
    "# Investigate \"initial_list_status\"\n",
    "get_feature_info('initial_list_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature only has 2 possible values, so make dummy variables\n",
    "initial_list_status_dummies = pd.get_dummies(df['initial_list_status'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('initial_list_status', axis=1, inplace=True)\n",
    "df = pd.concat([df, initial_list_status_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indicates whether the loan is an individual application or a joint application with two co-borrowers\n"
     ]
    }
   ],
   "source": [
    "# Investigate \"application_type\"\n",
    "get_feature_info('application_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INDIVIDUAL    394508\n",
       "JOINT            425\n",
       "DIRECT_PAY       286\n",
       "Name: application_type, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['application_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dummies\n",
    "app_type_dummies = pd.get_dummies(df['application_type'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, app_type_dummies], axis=1)\n",
    "df.drop('application_type', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['address'], dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check which non-numerical variables remain\n",
    "df.select_dtypes(['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       0174 Michelle Gateway\\nMendozaberg, OK 22690\n",
       "1    1076 Carney Fort Apt. 347\\nLoganmouth, SD 05113\n",
       "2    87025 Mark Dale Apt. 269\\nNew Sabrina, WV 05113\n",
       "Name: address, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate \"address\"\n",
    "print(df['address'].nunique())\n",
    "df['address'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"address\" proabably won't tell us much without more domain knowledge. Zip code may be helpful, but there is no ordinal encoding to these numbers, so they won't be useful as ints. Pehaps they could be useful as categories. See if the number of unique zip codes is small enough to encode them as categorical dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the zip codes from the address strings\n",
    "df['zip_code'] = df['address'].apply(lambda address: address.split()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70466    56880\n",
       "22690    56413\n",
       "30723    56402\n",
       "48052    55811\n",
       "00813    45725\n",
       "29597    45393\n",
       "05113    45300\n",
       "11650    11210\n",
       "93700    11126\n",
       "86630    10959\n",
       "Name: zip_code, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['zip_code'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are only 10 unique zip codes, which is actually a very managable number. Therefore, I will create dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_dummies = pd.get_dummies(df['zip_code'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, zip_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'address' and 'zip_code'\n",
    "df.drop(['address', 'zip_code'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dataset has been cleared of null values and all the categorical features have been encoded numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['loan_amnt', 'term', 'int_rate', 'installment', 'annual_inc',\n",
       "       'loan_status', 'dti', 'earliest_cr_line', 'open_acc', 'pub_rec',\n",
       "       'revol_bal', 'revol_util', 'total_acc', 'mort_acc',\n",
       "       'pub_rec_bankruptcies', 'emp_title_known', 'A2', 'A3', 'A4', 'A5', 'B1',\n",
       "       'B2', 'B3', 'B4', 'B5', 'C1', 'C2', 'C3', 'C4', 'C5', 'D1', 'D2', 'D3',\n",
       "       'D4', 'D5', 'E1', 'E2', 'E3', 'E4', 'E5', 'F1', 'F2', 'F3', 'F4', 'F5',\n",
       "       'G1', 'G2', 'G3', 'G4', 'G5', 'OTHER', 'OWN', 'RENT', 'Source Verified',\n",
       "       'Verified', 'credit_card', 'debt_consolidation', 'educational',\n",
       "       'home_improvement', 'house', 'major_purchase', 'medical', 'moving',\n",
       "       'other', 'renewable_energy', 'small_business', 'vacation', 'wedding',\n",
       "       'w', 'INDIVIDUAL', 'JOINT', '05113', '11650', '22690', '29597', '30723',\n",
       "       '48052', '70466', '86630', '93700'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns # The column names of the cleaned dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395219, 80)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that the data has been cleaned and entirely set to numerical features, I can begin splitting the data into training and test sets and build my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('loan_status', axis=1) # Select only the predictor features for the input data\n",
    "y = df['loan_status'].values # select the labels for your y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(395219, 79)\n",
      "(395219,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19615200686201828"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum() / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 20% of the loans in the dataset defaulted. This means that there is a class imbalance in the dataset. In order to make the model sensitive to this imbalance, I will pass a dict to the class_weight argument of the Keras .fit() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Split data into training, validation, and test sets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set stratify=y to maintain class balance across datasets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size = 0.1, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284557, 79)\n",
      "(71140, 79)\n",
      "(39522, 79)\n",
      "(284557,)\n",
      "(71140,)\n",
      "(39522,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1966706143233166\n",
      "0.19408209165026707\n",
      "0.19614391984211324\n"
     ]
    }
   ],
   "source": [
    "# Check that the class balance is roughly the same across sets\n",
    "print(y_train.sum() / len(y_train))\n",
    "print(y_valid.sum() / len(y_valid))\n",
    "print(y_test.sum() / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Normalize the input variables using MinMaxScaler*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# DO NOT FIT SCALER TO VALIDATION OR TEST DATA; ONLY TRANSFORM THESE DATASETS\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've added Batch Normalization layers to speed up training. The activations, kernel initializations, and optimizer are some defaults I usually start with that I discovered while reading the book *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(128, input_dim=X_train.shape[1], activation='elu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(256, activation='elu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(256, activation='elu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(128, activation='elu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(64, activation='elu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='nadam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Use Early Stopping to reduce overfitting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_cb = EarlyStopping(patience=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Define a class weights dictionary*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a class imbalance in the dataset, with far fewer instances of default than \"fully paid\". The ratio of default to fully paid is around 1 to 4, so if the classifier just guesses the \"fully paid\" class every time, it would achieve an accuracy of around 80%. However, as a lender, the company is primarily interested in being able to detect when an application is likely to result in a default. So the company doesn't care about the accuracy of the classifier as much as it cares about the classifier's positive class recall score, which is the percentage of loan defaults in the dataset that the model actually detects. \n",
    "\n",
    "This preference for detecting the minority class means the company is content with making false positive errors (misclassifying truly good loan applications as defaults) if it means that it can minimize false negative errors (misclassifying a loan application that will actually default as a good loan). This tradeoff will lead to a lower overall accuracy score in favor of a higher recall score.\n",
    "\n",
    "To emphasize the importance of classifying the positive (loan default) class correctly, I will pass a dictionary to the class_weight argument of the .fit() method. This dictionary will contain the class labels as keys and their weights/importances as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 1, 1: 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "8893/8893 [==============================] - 48s 5ms/step - loss: 0.9188 - accuracy: 0.6354 - val_loss: 0.5742 - val_accuracy: 0.5933\n",
      "Epoch 2/500\n",
      "8893/8893 [==============================] - 49s 5ms/step - loss: 0.8779 - accuracy: 0.6420 - val_loss: 0.5076 - val_accuracy: 0.6757\n",
      "Epoch 3/500\n",
      "8893/8893 [==============================] - 48s 5ms/step - loss: 0.8718 - accuracy: 0.6394 - val_loss: 0.5273 - val_accuracy: 0.6622\n",
      "Epoch 4/500\n",
      "8893/8893 [==============================] - 48s 5ms/step - loss: 0.8671 - accuracy: 0.6425 - val_loss: 0.5359 - val_accuracy: 0.6481\n",
      "Epoch 5/500\n",
      "8893/8893 [==============================] - 48s 5ms/step - loss: 0.8649 - accuracy: 0.6441 - val_loss: 0.4972 - val_accuracy: 0.6566\n",
      "Epoch 6/500\n",
      "8893/8893 [==============================] - 48s 5ms/step - loss: 0.8622 - accuracy: 0.6436 - val_loss: 0.5047 - val_accuracy: 0.6992\n",
      "Epoch 7/500\n",
      "8893/8893 [==============================] - 48s 5ms/step - loss: 0.8616 - accuracy: 0.6489 - val_loss: 0.5441 - val_accuracy: 0.6674\n",
      "Epoch 8/500\n",
      "8893/8893 [==============================] - 49s 6ms/step - loss: 0.8597 - accuracy: 0.6476 - val_loss: 0.5093 - val_accuracy: 0.6387\n",
      "Epoch 9/500\n",
      "8893/8893 [==============================] - 49s 5ms/step - loss: 0.8578 - accuracy: 0.6490 - val_loss: 0.5312 - val_accuracy: 0.6597\n",
      "Epoch 10/500\n",
      "8893/8893 [==============================] - 49s 5ms/step - loss: 0.8570 - accuracy: 0.6527 - val_loss: 0.5834 - val_accuracy: 0.6297\n",
      "Epoch 11/500\n",
      "8893/8893 [==============================] - 49s 5ms/step - loss: 0.8549 - accuracy: 0.6549 - val_loss: 0.5706 - val_accuracy: 0.6200\n",
      "Epoch 12/500\n",
      "8893/8893 [==============================] - 48s 5ms/step - loss: 0.8539 - accuracy: 0.6529 - val_loss: 0.5376 - val_accuracy: 0.6530\n",
      "Epoch 13/500\n",
      "8893/8893 [==============================] - 48s 5ms/step - loss: 0.8535 - accuracy: 0.6543 - val_loss: 0.5485 - val_accuracy: 0.6599\n",
      "Epoch 14/500\n",
      "8893/8893 [==============================] - 49s 5ms/step - loss: 0.8525 - accuracy: 0.6560 - val_loss: 0.4868 - val_accuracy: 0.7135\n",
      "Epoch 15/500\n",
      "8893/8893 [==============================] - 49s 5ms/step - loss: 0.8506 - accuracy: 0.6558 - val_loss: 0.5131 - val_accuracy: 0.6799\n",
      "Epoch 16/500\n",
      "8893/8893 [==============================] - 50s 6ms/step - loss: 0.8513 - accuracy: 0.6574 - val_loss: 0.5104 - val_accuracy: 0.6830\n",
      "Epoch 17/500\n",
      "8893/8893 [==============================] - 48s 5ms/step - loss: 0.8501 - accuracy: 0.6592 - val_loss: 0.5287 - val_accuracy: 0.6428\n",
      "Epoch 18/500\n",
      "8893/8893 [==============================] - 48s 5ms/step - loss: 0.8490 - accuracy: 0.6574 - val_loss: 0.5370 - val_accuracy: 0.6496\n",
      "Epoch 19/500\n",
      "8893/8893 [==============================] - 49s 5ms/step - loss: 0.8482 - accuracy: 0.6581 - val_loss: 0.4798 - val_accuracy: 0.7022\n",
      "Epoch 20/500\n",
      "8893/8893 [==============================] - 48s 5ms/step - loss: 0.8470 - accuracy: 0.6597 - val_loss: 0.5378 - val_accuracy: 0.6657\n",
      "Epoch 21/500\n",
      "8893/8893 [==============================] - 48s 5ms/step - loss: 0.8464 - accuracy: 0.6592 - val_loss: 0.5039 - val_accuracy: 0.6936\n",
      "Epoch 22/500\n",
      "8893/8893 [==============================] - 49s 6ms/step - loss: 0.8445 - accuracy: 0.6587 - val_loss: 0.5746 - val_accuracy: 0.6634\n",
      "Epoch 23/500\n",
      "8893/8893 [==============================] - 48s 5ms/step - loss: 0.8451 - accuracy: 0.6603 - val_loss: 0.5321 - val_accuracy: 0.6777\n",
      "Epoch 24/500\n",
      "8893/8893 [==============================] - 49s 5ms/step - loss: 0.8438 - accuracy: 0.6617 - val_loss: 0.5291 - val_accuracy: 0.7075\n",
      "Epoch 25/500\n",
      "8893/8893 [==============================] - 49s 5ms/step - loss: 0.8434 - accuracy: 0.6645 - val_loss: 0.5319 - val_accuracy: 0.6567\n",
      "Epoch 26/500\n",
      "8893/8893 [==============================] - 45s 5ms/step - loss: 0.8424 - accuracy: 0.6635 - val_loss: 0.5493 - val_accuracy: 0.6588\n",
      "Epoch 27/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8424 - accuracy: 0.6623 - val_loss: 0.4875 - val_accuracy: 0.7123\n",
      "Epoch 28/500\n",
      "8893/8893 [==============================] - 40s 5ms/step - loss: 0.8411 - accuracy: 0.6634 - val_loss: 0.5851 - val_accuracy: 0.6538\n",
      "Epoch 29/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.8409 - accuracy: 0.6634 - val_loss: 0.5508 - val_accuracy: 0.6939\n",
      "Epoch 30/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8409 - accuracy: 0.6657 - val_loss: 0.5921 - val_accuracy: 0.6440\n",
      "Epoch 31/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8381 - accuracy: 0.6643 - val_loss: 0.5396 - val_accuracy: 0.6814\n",
      "Epoch 32/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8388 - accuracy: 0.6655 - val_loss: 0.5435 - val_accuracy: 0.6731\n",
      "Epoch 33/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.8377 - accuracy: 0.6672 - val_loss: 0.5480 - val_accuracy: 0.6703\n",
      "Epoch 34/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8380 - accuracy: 0.6647 - val_loss: 0.5398 - val_accuracy: 0.7010\n",
      "Epoch 35/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8362 - accuracy: 0.6678 - val_loss: 0.5744 - val_accuracy: 0.6813\n",
      "Epoch 36/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8366 - accuracy: 0.6656 - val_loss: 0.5951 - val_accuracy: 0.6699\n",
      "Epoch 37/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.8356 - accuracy: 0.6663 - val_loss: 0.6131 - val_accuracy: 0.6545\n",
      "Epoch 38/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.8339 - accuracy: 0.6700 - val_loss: 0.6564 - val_accuracy: 0.6794\n",
      "Epoch 39/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8342 - accuracy: 0.6697 - val_loss: 0.6432 - val_accuracy: 0.6580\n",
      "Epoch 40/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8334 - accuracy: 0.6702 - val_loss: 0.7421 - val_accuracy: 0.6367\n",
      "Epoch 41/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.8323 - accuracy: 0.6677 - val_loss: 0.6368 - val_accuracy: 0.6732\n",
      "Epoch 42/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8321 - accuracy: 0.6720 - val_loss: 0.6773 - val_accuracy: 0.6867\n",
      "Epoch 43/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8318 - accuracy: 0.6705 - val_loss: 0.6300 - val_accuracy: 0.6496\n",
      "Epoch 44/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8311 - accuracy: 0.6724 - val_loss: 0.7071 - val_accuracy: 0.6881\n",
      "Epoch 45/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8304 - accuracy: 0.6717 - val_loss: 0.7121 - val_accuracy: 0.6573\n",
      "Epoch 46/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8280 - accuracy: 0.6723 - val_loss: 0.6117 - val_accuracy: 0.6674\n",
      "Epoch 47/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8281 - accuracy: 0.6725 - val_loss: 0.5871 - val_accuracy: 0.6717\n",
      "Epoch 48/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8277 - accuracy: 0.6720 - val_loss: 0.6138 - val_accuracy: 0.6786\n",
      "Epoch 49/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8271 - accuracy: 0.6747 - val_loss: 0.7012 - val_accuracy: 0.6472\n",
      "Epoch 50/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8264 - accuracy: 0.6740 - val_loss: 0.6706 - val_accuracy: 0.6434\n",
      "Epoch 51/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8270 - accuracy: 0.6736 - val_loss: 0.5498 - val_accuracy: 0.7063\n",
      "Epoch 52/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.8266 - accuracy: 0.6758 - val_loss: 0.5551 - val_accuracy: 0.7000\n",
      "Epoch 53/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8242 - accuracy: 0.6749 - val_loss: 0.6206 - val_accuracy: 0.6734\n",
      "Epoch 54/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8235 - accuracy: 0.6752 - val_loss: 0.6511 - val_accuracy: 0.6649\n",
      "Epoch 55/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8224 - accuracy: 0.6735 - val_loss: 0.5893 - val_accuracy: 0.6782\n",
      "Epoch 56/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8237 - accuracy: 0.6781 - val_loss: 0.6832 - val_accuracy: 0.6676\n",
      "Epoch 57/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.8223 - accuracy: 0.6746 - val_loss: 0.6564 - val_accuracy: 0.6640\n",
      "Epoch 58/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.8206 - accuracy: 0.6757 - val_loss: 0.6491 - val_accuracy: 0.6600\n",
      "Epoch 59/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8199 - accuracy: 0.6797 - val_loss: 0.6315 - val_accuracy: 0.6837\n",
      "Epoch 60/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8182 - accuracy: 0.6811 - val_loss: 0.7036 - val_accuracy: 0.6692\n",
      "Epoch 61/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8187 - accuracy: 0.6809 - val_loss: 0.7686 - val_accuracy: 0.6377\n",
      "Epoch 62/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8176 - accuracy: 0.6792 - val_loss: 0.7710 - val_accuracy: 0.6355\n",
      "Epoch 63/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8160 - accuracy: 0.6785 - val_loss: 0.7743 - val_accuracy: 0.6416\n",
      "Epoch 64/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8162 - accuracy: 0.6796 - val_loss: 0.7765 - val_accuracy: 0.6524\n",
      "Epoch 65/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8152 - accuracy: 0.6808 - val_loss: 0.8436 - val_accuracy: 0.6581\n",
      "Epoch 66/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.8144 - accuracy: 0.6830 - val_loss: 0.8672 - val_accuracy: 0.6918\n",
      "Epoch 67/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8130 - accuracy: 0.6847 - val_loss: 0.8951 - val_accuracy: 0.6537\n",
      "Epoch 68/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8136 - accuracy: 0.6860 - val_loss: 0.6750 - val_accuracy: 0.6586\n",
      "Epoch 69/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8128 - accuracy: 0.6846 - val_loss: 0.7872 - val_accuracy: 0.7073\n",
      "Epoch 70/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8124 - accuracy: 0.6855 - val_loss: 0.7768 - val_accuracy: 0.6677\n",
      "Epoch 71/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8103 - accuracy: 0.6877 - val_loss: 0.7379 - val_accuracy: 0.6814\n",
      "Epoch 72/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8105 - accuracy: 0.6870 - val_loss: 0.6758 - val_accuracy: 0.6817\n",
      "Epoch 73/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8079 - accuracy: 0.6871 - val_loss: 0.8133 - val_accuracy: 0.6603\n",
      "Epoch 74/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8069 - accuracy: 0.6881 - val_loss: 0.6964 - val_accuracy: 0.6673\n",
      "Epoch 75/500\n",
      "8893/8893 [==============================] - 41s 5ms/step - loss: 0.8070 - accuracy: 0.6887 - val_loss: 0.6786 - val_accuracy: 0.6643\n",
      "Epoch 76/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.8064 - accuracy: 0.6865 - val_loss: 0.6934 - val_accuracy: 0.6724\n",
      "Epoch 77/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8052 - accuracy: 0.6884 - val_loss: 0.6742 - val_accuracy: 0.6754\n",
      "Epoch 78/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8044 - accuracy: 0.6894 - val_loss: 0.7008 - val_accuracy: 0.6672\n",
      "Epoch 79/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8044 - accuracy: 0.6909 - val_loss: 0.7263 - val_accuracy: 0.6842\n",
      "Epoch 80/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.8036 - accuracy: 0.6886 - val_loss: 0.8297 - val_accuracy: 0.6627\n",
      "Epoch 81/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8040 - accuracy: 0.6897 - val_loss: 0.9502 - val_accuracy: 0.6707\n",
      "Epoch 82/500\n",
      "8893/8893 [==============================] - 39s 4ms/step - loss: 0.8013 - accuracy: 0.6914 - val_loss: 0.8217 - val_accuracy: 0.6859\n",
      "Epoch 83/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.7999 - accuracy: 0.6921 - val_loss: 0.8902 - val_accuracy: 0.6624\n",
      "Epoch 84/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.7998 - accuracy: 0.6914 - val_loss: 0.7228 - val_accuracy: 0.6665\n",
      "Epoch 85/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.7988 - accuracy: 0.6931 - val_loss: 0.7664 - val_accuracy: 0.6824\n",
      "Epoch 86/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.8003 - accuracy: 0.6907 - val_loss: 0.6748 - val_accuracy: 0.6930\n",
      "Epoch 87/500\n",
      "8893/8893 [==============================] - 40s 5ms/step - loss: 0.7981 - accuracy: 0.6926 - val_loss: 1.5259 - val_accuracy: 0.6765\n",
      "Epoch 88/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.7977 - accuracy: 0.6926 - val_loss: 0.8985 - val_accuracy: 0.6884\n",
      "Epoch 89/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.7966 - accuracy: 0.6930 - val_loss: 1.1558 - val_accuracy: 0.6617\n",
      "Epoch 90/500\n",
      "8893/8893 [==============================] - 40s 5ms/step - loss: 0.7955 - accuracy: 0.6966 - val_loss: 2.3736 - val_accuracy: 0.6663\n",
      "Epoch 91/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.7952 - accuracy: 0.6961 - val_loss: 0.7637 - val_accuracy: 0.6807\n",
      "Epoch 92/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.7945 - accuracy: 0.6961 - val_loss: 0.8412 - val_accuracy: 0.6660\n",
      "Epoch 93/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.7940 - accuracy: 0.6971 - val_loss: 1.0824 - val_accuracy: 0.6789\n",
      "Epoch 94/500\n",
      "8893/8893 [==============================] - 40s 4ms/step - loss: 0.7920 - accuracy: 0.6978 - val_loss: 0.9162 - val_accuracy: 0.6664\n"
     ]
    }
   ],
   "source": [
    "# Train for 500 epochs with early stopping; pass the class_weight dict as an argument so the model is sensitive\n",
    "# of the class imbalance\n",
    "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_valid, y_valid), class_weight=class_weight,\n",
    "                    callbacks=[early_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('lending_club_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Evaluate the model's performance on the training and evaluate its generalization on the test set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.62      0.76    228593\n",
      "           1       0.38      0.93      0.54     55964\n",
      "\n",
      "    accuracy                           0.69    284557\n",
      "   macro avg       0.68      0.78      0.65    284557\n",
      "weighted avg       0.86      0.69      0.72    284557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_classes(X_train)\n",
    "print(classification_report(y_train, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.62      0.75     31770\n",
      "           1       0.36      0.89      0.51      7752\n",
      "\n",
      "    accuracy                           0.67     39522\n",
      "   macro avg       0.66      0.75      0.63     39522\n",
      "weighted avg       0.84      0.67      0.70     39522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_classes(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19549 12221]\n",
      " [  861  6891]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model only achieved 69% accuracy on the training set and 67% accuracy on the test set. However, remember that accuracy is not the most important metric for this task. The most important metric is recall. Recall is the proportion of positive instances in the dataset that the model detects and correctly classifies as positive. In this case, recall means the proportion of defaults in the dataset that were correctly flagged by the model. Defaults are expensive for a lending company, so high recall is important because it means the company will be able to identify more loans that actually default and save itself money. On the test set, this model achieved 89% recall, which means that out of all the defaulted loans in the unseen data, the model detected 89% of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the data into the proper form, I build a Keras neural net model that could classify loans as either likely to be fully repaid or likely to default. After being trained for 94 epochs, the model achieved fairly mediocre accuracies of 69% and 67% on the training and test sets respectively. However, accuracy is not the most important metric because the dataset had a class imbalance and the cost of false positives and false negatives was not equal. Loan defaults are expensive to a lender, so it wants to make as few false negative errors (misclassifying a loan application that will actually default as a good loan) as possible. Therefore, the proper measure of this classifier's performance is its positive class recall. On this metric, the model achieved 93% and 89% recall on the training and test sets respectively. This means that the model is good at detecting the loans that will actually default.\n",
    "\n",
    "This high recall score did come at the expense of the overall accuracy of the classifier. I ran this model a few times before adding class weights to emphasize the positive class, and these models achieved accuracies over 90% but had recall scores between 40%-50%. Given that the task of the model is to identify good and bad loans, this recall was simply too low. When I added class weights, the recall increased drastically at the expense of the overall accuracy. This is because the model started to flag more instances as likely defaults, leading to more false positive errors but fewer false negatives. This tradeoff should be fine though given the real world application of this model. A lender would rather have its software incorrectly flag a loan as a likely default then have a human analyst give it a second opinion than have the software incorrectly say the loan is good then end up having the borrower default.\n",
    "\n",
    "One final note, training set recall reached 93% by the time the early stopping callback stopped the model's training. I could go back and continue to let the model train and its recall would continue to increase, but its accuracy would likely start falling more sharply. At this point it would be up the the end user of this system to choose the ideal recall-precision-accuracy tradeoff and proceed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
